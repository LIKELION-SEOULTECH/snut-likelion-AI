from sentence_transformers import SentenceTransformer, util
from transformers import AutoTokenizer, AutoModelForCausalLM
import pandas as pd
import torch

# 1. ê²€ìƒ‰ìš© SBERT ë¡œë“œ
sbert = SentenceTransformer("./sbert_finetuned")

# 2. ìƒì„±ìš© fine-tuned KoGPT ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("./kogpt_finetuned")
generator = AutoModelForCausalLM.from_pretrained("./kogpt_finetuned", device_map="auto")

# 3. ë°ì´í„° ë¡œë”© ë° ì§ˆë¬¸ ì„ë² ë”©
df = pd.read_excel("chatbot_augmented.xlsx")
questions = df["ì§ˆë¬¸"].tolist()
answers = df["ë‹µë³€"].tolist()
question_embeddings = sbert.encode(questions, convert_to_tensor=True)


# 4. ìœ ì‚¬ ì§ˆë¬¸ ê²€ìƒ‰ í•¨ìˆ˜
def find_similar(user_input, top_k=1):
    input_embedding = sbert.encode(user_input, convert_to_tensor=True)
    cos_scores = util.cos_sim(input_embedding, question_embeddings)[0]
    top_result = torch.topk(cos_scores, k=top_k)
    best_idx = top_result.indices[0].item()
    score = top_result.values[0].item()
    return questions[best_idx], answers[best_idx], score


# 5. ìƒì„± í•¨ìˆ˜
def generate_answer(user_input, matched_question, matched_answer):
    prompt = f"""ë‹¹ì‹ ì€ ì„œìš¸ê³¼í•™ê¸°ìˆ ëŒ€í•™êµ ë©‹ìŸì´ì‚¬ìì²˜ëŸ¼ ë™ì•„ë¦¬ì˜ ê³µì‹ ì•ˆë‚´ ì±—ë´‡ì…ë‹ˆë‹¤.  
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ì— ì œê³µëœ ê¸°ì¡´ ë‹µë³€ì„ ì°¸ê³ í•˜ì—¬, ë™ì¼í•œ ë‚´ìš©ì„ ìì—°ìŠ¤ëŸ½ê³  ì¹œì ˆí•œ ë§íˆ¬ë¡œ ì „ë‹¬í•´ ì£¼ì„¸ìš”.  
ì‚¬ì‹¤ì— ê¸°ë°˜í•œ ì‘ë‹µë§Œ ì œê³µí•˜ë©°, ìƒˆë¡œìš´ ì •ë³´ë¥¼ ì„ì˜ë¡œ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”. 

[ì‚¬ìš©ì ì§ˆë¬¸]
{user_input}

[ê¸°ì¡´ ë‹µë³€]
{matched_answer}

[ì‘ë‹µ]
"""
    inputs = tokenizer(
        prompt, return_tensors="pt", padding=True, truncation=True, max_length=512
    )
    input_ids = inputs["input_ids"].to(generator.device)
    attention_mask = inputs["attention_mask"].to(generator.device)

    output = generator.generate(
        input_ids=input_ids,
        attention_mask=attention_mask,
        max_new_tokens=150,
        do_sample=True,
        temperature=0.7,
        pad_token_id=tokenizer.eos_token_id,
    )

    # ğŸŸ¡ í›„ì²˜ë¦¬: ì˜ëª»ëœ ë„ì–´ì“°ê¸° ê³ ì¹˜ê¸°
    raw_output = tokenizer.decode(output[0], skip_special_tokens=True)
    response = raw_output.split("[ì‘ë‹µ]")[-1].strip()

    # ğŸ”§ ë„ì–´ì“°ê¸° ìë™ ìˆ˜ì •
    fixes = {
        "ë©‹ì‚¬ ì€": "ë©‹ì‚¬ì€",
        "ë©‹ìŸì´ì‚¬ìì²˜ëŸ¼ ì€": "ë©‹ìŸì´ì‚¬ìì²˜ëŸ¼ì€",
        "ë©‹ì‚¬ ëŠ”": "ë©‹ì‚¬ëŠ”",
        "ë©‹ì‚¬ ì˜": "ë©‹ì‚¬ì˜",
    }
    for wrong, correct in fixes.items():
        response = response.replace(wrong, correct)

    return response


# 6. í†µí•© ì±—ë´‡ í•¨ìˆ˜
def hybrid_chatbot(user_input, fallback_threshold=0.5):
    matched_q, matched_a, score = find_similar(user_input)
    if score < fallback_threshold:
        return "ì •í™•í•œ ì •ë³´ë¥¼ ì°¾ê¸° ì–´ë ¤ì›Œìš”. ì¸ìŠ¤íƒ€ DMìœ¼ë¡œ ë¬¸ì˜ ì£¼ì„¸ìš” ğŸ‘‰ instagram.com/likelion_st"
    return generate_answer(user_input, matched_q, matched_a)


# 7. í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    test_questions = [
        "ì„¸ì…˜ì€ ê¼­ ì°¸ì—¬í•´ì•¼ í•˜ë‚˜ìš”?",
        "í•´ì»¤í†¤ì€ ê¼­ ë‚˜ê°€ì•¼ ë¼?",
        "ì¤‘ì•™ OTëŠ” í•„ìˆ˜ì¸ê°€ìš”?",
        "ì„¸ì…˜ ë¹ ì§€ë©´ ìˆ˜ë£Œ ëª»í•˜ë‚˜ìš”?",
        "ë™ì•„ë¦¬ í™œë™ ì‹œê°„ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
        "ìŠ¤í„°ë””ëŠ” ì˜ë¬´ì¸ê°€ìš”?",
    ]

    for q in test_questions:
        print("ğŸ‘¤ ì‚¬ìš©ì ì§ˆë¬¸:", q)
        matched_q, matched_a, score = find_similar(q)
        print("ğŸ” ê°€ì¥ ìœ ì‚¬í•œ ì§ˆë¬¸:", matched_q)
        print("ğŸ“Œ ê¸°ì¡´ ë‹µë³€:", matched_a)
        print("ğŸ¤– ìƒì„±ëœ ë‹µë³€:")
        print(generate_answer(q, matched_q, matched_a))
        print("-" * 60)
